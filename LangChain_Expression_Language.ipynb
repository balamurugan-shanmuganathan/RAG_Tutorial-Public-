{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Expression Language (LECL)\n",
        "\n",
        "LangChain Expression Language (LEL) is a **declarative scripting language** used in the LangChain framework. It allows developers to define **sequences of operations** for processing and managing language models, data, and interactions with other systems.\n",
        "\n",
        "\n",
        "Ref link:\n",
        "\n",
        "\n",
        "\n",
        "**Introduction to LCEL (LangChain Expression Language)**\n",
        "\n",
        "https://youtu.be/POVaB5AfzC4\n",
        "\n",
        "\n",
        "\n",
        "**LangChain Expression Language (LCEL) | Langchain Tutorial | Code**\n",
        "\n",
        "https://youtu.be/NQWfvhw7OcI\n",
        "\n",
        "\n",
        "**LangChain Expression language(LCEL) for Chaining the Components | All Runnables | Async & Streaming**\n",
        "\n",
        "https://youtu.be/8aUYzb1aYDU"
      ],
      "metadata": {
        "id": "UCzFnmMAKRHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "Z0xvWwuGUInv"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain_google_genai\n",
        "!pip install -q langchain_community\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain_huggingface\n",
        "!pip install -q chromadb\n",
        "!pip install -q tiktoken\n",
        "!pip install -q bs4\n",
        "!pip install -q python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Environment"
      ],
      "metadata": {
        "id": "axW_eE7MNGdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
        "import bs4\n"
      ],
      "metadata": {
        "id": "cvU-HOpHIXxB"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "google_llm = ChatGoogleGenerativeAI(model='gemini-1.0-pro')\n",
        "\n",
        "## Huggingface Embeddings\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO9yHKctJcKN",
        "outputId": "c60bfee1-2c3c-4f46-e81e-63fab5af6515"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=[\n",
        "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "    ],\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\",\"post-header\")\n",
        "        )\n",
        "    )\n",
        ")\n",
        "document = loader.load()"
      ],
      "metadata": {
        "id": "WBeQh-bjKLAp"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Text Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "splits = text_splitter.split_documents(document)\n",
        "print(\"No of chunks : \", len(splits))\n",
        "splits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWvepMXHL2J0",
        "outputId": "d1171f07-086e-4f7d-e25f-6e87bc48cfdc"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of chunks :  31\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.')"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## VectoreStore\n",
        "vectorestore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=hf_embeddings\n",
        ")\n",
        "vectorestore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNZS8FEfMsE8",
        "outputId": "8112a88b-7d17-4e2d-f902-06431d40c373"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7c7968f87cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Retriever\n",
        "retriever = vectorestore.as_retriever()\n",
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJMCWkJJM-Iv",
        "outputId": "f46adba3-b9a2-4ef4-8bee-2759b8a1fc73"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7c7968f87cd0>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LECL\n",
        "\n",
        "### Topic Covered\n",
        "\n",
        "1. **Invoke** - Invoke chain with input\n",
        "\n",
        "2. **Batch** - Invoke chain with list of inputs\n",
        "\n",
        "3. **Stream** - Streaming the response\n",
        "\n",
        "4. **RunnablePassthrough** - Feed input to chain\n",
        "\n",
        "5. **RunnableParallel** - Parallel execution\n",
        "\n",
        "6. **RunnableLambda** - Use user_defined_functions\n",
        "\n",
        "7. **ainvoke** - asynchronously invokes chain with input\n",
        "\n",
        "8. **abatch** - asynchronously invokes chain with list of inputs"
      ],
      "metadata": {
        "id": "u0y7aVwnH3u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Provide me the 5 summary line of the following topic from the context.\n",
        "{context}\n",
        "\n",
        "topic : {topic}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "iO-9xCGuUP50"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | google_llm\n",
        "\n",
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiTKFNsWIq8",
        "outputId": "cdc861de-546f-422f-d74a-723fc9db1c9f"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'topic'], input_types={}, partial_variables={}, template='Provide me the 5 summary line of the following topic from the context.\\n{context}\\n\\ntopic : {topic}\\n'), additional_kwargs={})])\n",
              "| ChatGoogleGenerativeAI(model='models/gemini-1.0-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7c796914d060>, default_metadata=())"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoke method"
      ],
      "metadata": {
        "id": "y_dVHl21PsL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Simple Invoke method\n",
        "chain = prompt | google_llm\n",
        "\n",
        "context_ = retriever.get_relevant_documents (\"Task Decomposition\")\n",
        "chain.invoke({'context' : context_,'topic' : 'Task Decomposition'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDL24hrwUSbC",
        "outputId": "f2e2377d-4085-4f5d-b981-9d5f368c7acc"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='1. Task decomposition is crucial for planning complex tasks in autonomous agent systems.\\n2. Chain of thought (CoT) and Tree of Thoughts (ToT) are prompting techniques that facilitate task decomposition by breaking down problems into smaller steps.\\n3. Task decomposition can be performed by LLM with simple prompts, task-specific instructions, or human inputs.\\n4. LLM+P (LLM and Planner) leverages an external classical planner to handle long-horizon planning through PDDL (Planning Domain Definition Language).\\n5. Task decomposition enables agents to generate a plan of action, identify subgoals, and efficiently execute complex tasks.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-1a90bbfd-59a6-47bf-afe2-012c1a92152a-0', usage_metadata={'input_tokens': 2026, 'output_tokens': 129, 'total_tokens': 2155})"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## StrOutputParser\n",
        "chain = prompt | google_llm | StrOutputParser()\n",
        "\n",
        "context_ = retriever.get_relevant_documents (\"Task Decomposition\")\n",
        "chain.invoke({'context' : context_,'topic' : 'Task Decomposition'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ZhFwpzGFUaMi",
        "outputId": "3f3304dc-2dcd-4cf6-836a-48e5904c06fd"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. Task Decomposition is a key component of LLM-powered autonomous agents, enabling them to break down complex tasks into manageable steps.\\n2. Chain of Thought (CoT) and Tree of Thoughts (ToT) are prompting techniques that guide LLMs to decompose tasks step-by-step, providing insights into their reasoning process.\\n3. Task decomposition can be performed by LLMs with simple prompting, task-specific instructions, or human inputs.\\n4. LLM+P is an alternative approach that utilizes an external classical planner to generate a plan in the Planning Domain Definition Language (PDDL), which is then translated by the LLM.\\n5. The Planning stage in an LLM-powered agent system involves task parsing, model selection, and task execution, with LLMs playing a central role in coordinating these processes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## StrOutputParser + Split the line function\n",
        "chain = prompt | google_llm | StrOutputParser() | (lambda x : x.split('\\n'))\n",
        "\n",
        "context_ = retriever.get_relevant_documents (\"Task Decomposition\")\n",
        "chain.invoke({'context' : context_,'topic' : 'Task Decomposition'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSGlg4mlOuZp",
        "outputId": "a7bd0380-fde3-4bd1-9848-1657f81cd582"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. Task decomposition is essential for complex tasks, allowing an agent to plan ahead by breaking them into smaller steps.',\n",
              " \"2. Chain of Thought (CoT) and Tree of Thoughts extend decomposition by providing a step-by-step interpretation of the model's reasoning process.\",\n",
              " '3. Task decomposition can be performed by the LLM itself, through external instructions, or with human input.',\n",
              " '4. LLM+P involves outsourcing planning to an external planner using the Planning Domain Definition Language (PDDL).',\n",
              " '5. Self-reflection enhances planning by allowing the agent to evaluate and refine its task decomposition strategies.']"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch"
      ],
      "metadata": {
        "id": "Am3jKlSaPyXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## StrOutputParser + Split the line function\n",
        "chain = prompt | google_llm | StrOutputParser() | (lambda x: x.split('\\n'))\n",
        "\n",
        "context_sensory_memory = retriever.get_relevant_documents (\"Sensory Memory\")\n",
        "context_short_term_memory = retriever.get_relevant_documents (\"Short-Term Memory\")\n",
        "context_long_term_memory = retriever.get_relevant_documents (\"Long-Term Memory\")\n",
        "context_mips = retriever.get_relevant_documents (\"Maximum Inner Product Search (MIPS)\")\n",
        "\n",
        "chain.batch([\n",
        "    {'context' : context_sensory_memory,'topic' : 'Sensory Memory'},\n",
        "    {'context' : context_short_term_memory,'topic' : 'Short-Term Memory'},\n",
        "    {'context' : context_long_term_memory,'topic' : 'Long-Term Memory'},\n",
        "    {'context' : context_mips,'topic' : 'Maximum Inner Product Search (MIPS)'},\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXHDKi8tPxld",
        "outputId": "4bb79d0c-d148-4386-d4af-f67d9f8b90a4"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['1. Sensory memory is the initial stage of memory that stores impressions of sensory information for a few seconds.',\n",
              "  '2. It includes iconic memory (visual), echoic memory (auditory), and haptic memory (touch).',\n",
              "  '3. Sensory memory is analogous to learning embedding representations for raw inputs in AI models.',\n",
              "  '4. The short-term memory capacity is limited and lasts for approximately 20-30 seconds.',\n",
              "  '5. Short-term memory is comparable to in-context learning in AI models, which is restricted by the finite context window length.'],\n",
              " ['1. Short-term memory (STM) stores information currently in use and is essential for cognitive tasks.',\n",
              "  '2. STM has a limited capacity of approximately 7 items and lasts for 20-30 seconds.',\n",
              "  '3. In AI, sensory memory is analogous to embedding representations of raw inputs, while STM corresponds to in-context learning within a finite context window.',\n",
              "  '4. Long-term memory is represented as an external vector store accessible via fast retrieval, alleviating the restrictions of finite attention span.',\n",
              "  '5. Maximum Inner Product Search (MIPS) is a technique for fast retrieval of nearest neighbors from the external vector store, optimizing memory efficiency.'],\n",
              " ['1. Sensory memory captures raw inputs and creates embedding representations.',\n",
              "  '2. Short-term memory focuses on in-context learning within a limited context window.',\n",
              "  '3. Long-term memory serves as an external vector store accessible via fast retrieval.',\n",
              "  '4. Maximum Inner Product Search (MIPS) enables efficient retrieval of nearest neighbors from the external memory.',\n",
              "  '5. Approximate nearest neighbors (ANN) algorithm optimizes MIPS speed, sacrificing some accuracy for significant acceleration.'],\n",
              " ['1. MIPS (Maximum Inner Product Search) is a technique for retrieving similar vectors from a large vector store database.',\n",
              "  '2. MIPS is used to alleviate the finite attention span of artificial intelligence (AI) models, allowing them to access long-term memory.',\n",
              "  '3. ANN (Approximate Nearest Neighbors) algorithms are commonly used to optimize MIPS retrieval speed, trading accuracy for efficiency.',\n",
              "  '4. Common ANN algorithms for fast MIPS include Faiss and NMSLIB.',\n",
              "  '5. Performance comparisons of MIPS algorithms can be found on the ann-benchmarks.com website.']]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stream - Streaming the output"
      ],
      "metadata": {
        "id": "seJ3-rOuSjtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Provide me the 500 words summary of the following topic from the context.\n",
        "{context}\n",
        "\n",
        "topic : {topic}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | google_llm | StrOutputParser()\n",
        "\n",
        "context_ = retriever.get_relevant_documents (\"Task Decomposition\")\n",
        "\n",
        "\n",
        "for s in chain.stream({'context' : context_,'topic' : 'Task Decomposition'}):\n",
        "  print(s, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLViHVKyQmy6",
        "outputId": "81d740bc-30ef-4a87-f337-1c563a1046b8"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Task Decomposition for Autonomous Agents**\n",
            "\n",
            "Task decomposition is a crucial aspect of planning for autonomous agents, as it enables them to break down complex tasks into smaller, manageable steps. This process involves identifying the subtasks that need to be completed and the order in which they should be executed.\n",
            "\n",
            "**Chain of Thought (CoT)**\n",
            "\n",
            "CoT is a widely adopted prompting technique that enhances model performance on complex tasks. It instructs the model to \"think step by step,\" decomposing hard tasks into smaller steps by utilizing more test-time computation. CoT transforms large tasks into multiple manageable ones, offering insights into the model's thought process.\n",
            "\n",
            "**Tree of Thoughts (ToT)**\n",
            "\n",
            "ToT extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS or DFS, with each state evaluated by a classifier or majority vote.\n",
            "\n",
            "**Other Task Decomposition Approaches**\n",
            "\n",
            "Besides CoT and ToT, task decomposition can also be achieved through:\n",
            "\n",
            "* **LLM prompting:** Using simple prompts like \"Steps for XYZ\" or \"Subgoals for achieving XYZ.\"\n",
            "* **Task-specific instructions:** Providing specific instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\n",
            "* **Human inputs:** Manually breaking down the task into subtasks.\n",
            "\n",
            "**LLM+P Approach**\n",
            "\n",
            "LLM+P relies on an external classical planner to perform long-horizon planning. It utilizes the Planning Domain Definition Language (PDDL) to describe the planning problem. The LLM translates the problem into \"Problem PDDL,\" requests a plan from the planner based on \"Domain PDDL,\" and translates the PDDL plan back into natural language.\n",
            "\n",
            "**Example: HuggingGPT**\n",
            "\n",
            "HuggingGPT employs task decomposition in its four-stage process:\n",
            "\n",
            "1. **Task planning:** LLM parses user requests into tasks, identifying task type, ID, dependencies, and arguments.\n",
            "2. **Model selection:** LLM distributes tasks to expert models based on task type filtration.\n",
            "3. **Task execution:** Expert models execute the tasks and log results.\n",
            "4. **Result integration:** LLM combines results and generates a response to the user.\n",
            "\n",
            "**Benefits of Task Decomposition**\n",
            "\n",
            "Task decomposition offers several benefits for autonomous agents:\n",
            "\n",
            "* **Improved planning:** Breaking down tasks into smaller steps allows agents to plan more effectively and efficiently.\n",
            "* **Enhanced performance:** Decomposed tasks can be executed by specialized models, optimizing performance.\n",
            "* **Transparency:** Task decomposition provides insights into the agent's decision-making process, enhancing transparency and accountability."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Protocol"
      ],
      "metadata": {
        "id": "CAvHszSEUQ5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Provide me the 100 words summary of the following topic from the context.\n",
        "{context}\n",
        "\n",
        "topic : {topic}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "aIrQNPINTF34"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RunnablePassthrough"
      ],
      "metadata": {
        "id": "66s8eeg8UmH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | google_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke('Maximum Inner Product Search (MIPS)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "88BC-h4ZUao0",
        "outputId": "04a9b4d0-ac12-4815-e000-24f4f54e658b"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Maximum Inner Product Search (MIPS) is a technique that enables fast retrieval of relevant information from an external vector store. ANN algorithms, such as approximate nearest neighbors, are commonly used for MIPS to efficiently return the top k nearest neighbors based on inner product similarity. This allows agents to access a vast amount of information beyond their immediate context, extending their capabilities and alleviating the limitations of finite attention span.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | google_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.batch(['Task Decomposition', 'Types of Memory','Maximum Inner Product Search (MIPS)'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NrXme81UyFd",
        "outputId": "8d5a2ec4-f3ed-4241-e142-49a8fc673be2"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Task decomposition is a technique used in planning for complex tasks. It involves breaking down the task into smaller, manageable steps. This can be done using various methods, such as chain of thought (CoT), tree of thoughts, LLM+P, or simple prompting. CoT instructs the model to \"think step by step\" to utilize test-time computation to decompose hard tasks into smaller steps. Tree of thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. LLM+P relies on an external classical planner to do long-horizon planning, utilizing the Planning Domain Definition Language (PDDL). Simple prompting involves using LLM to decompose the task with prompts like \"Steps for XYZ.\\\\\\\\n1.\" or \"What are the subgoals for achieving XYZ?\". Task decomposition helps agents plan ahead and understand the steps involved in completing a task.',\n",
              " \"Memory involves acquiring, storing, and retrieving information. Human memory is categorized into:\\n\\n* **Sensory Memory**: Retains sensory information briefly (seconds).\\n* **Short-Term Memory (STM)**: Stores information we're currently aware of for complex tasks (20-30 seconds).\\n* **Long-Term Memory (LTM)**: Stores information for an extended period (days to decades). LTM has two subtypes:\\n    * **Explicit Memory**: Can be consciously recalled (episodic and semantic memory).\\n    * **Implicit Memory**: Involves skills and routines performed automatically.\",\n",
              " 'Maximum Inner Product Search (MIPS) is a technique used in artificial intelligence to efficiently retrieve information from a large database of vectors. By storing vector representations of information in an external memory, agents can access and attend to them during query time. To optimize retrieval speed, approximate nearest neighbor (ANN) algorithms are commonly employed, trading off some accuracy for significant speed improvements. Common ANN algorithms used for fast MIPS include:\\n- Locality-Sensitive Hashing (LSH)\\n- Product Quantization (PQ)\\n- Hierarchical Navigable Small World (HNSW)']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RunnableLambda"
      ],
      "metadata": {
        "id": "FpLlN3p1aP95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def string_upper(input: str) -> str:\n",
        "    return input.upper()\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | google_llm\n",
        "    | StrOutputParser()\n",
        "    | RunnableLambda(string_upper)\n",
        ")\n",
        "\n",
        "chain.batch(['Task Decomposition', 'Types of Memory','Maximum Inner Product Search (MIPS)'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH-RyuiLaPQ6",
        "outputId": "3e19574e-15e1-44ad-9654-bcca1dc12900"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TASK DECOMPOSITION IS A CRUCIAL ASPECT OF PLANNING FOR AUTONOMOUS AGENTS, AS IT ALLOWS COMPLEX TASKS TO BE BROKEN DOWN INTO SMALLER, MORE MANAGEABLE STEPS. CHAIN OF THOUGHT (COT) IS A STANDARD PROMPTING TECHNIQUE THAT ENHANCES MODEL PERFORMANCE ON COMPLEX TASKS BY INSTRUCTING THE MODEL TO \"THINK STEP BY STEP.\" TREE OF THOUGHTS EXTENDS COT BY EXPLORING MULTIPLE REASONING POSSIBILITIES AT EACH STEP. TASK DECOMPOSITION CAN BE DONE BY LLM WITH SIMPLE PROMPTING, TASK-SPECIFIC INSTRUCTIONS, OR HUMAN INPUTS. A DISTINCT APPROACH, LLM+P, INVOLVES RELYING ON AN EXTERNAL CLASSICAL PLANNER TO DO LONG-HORIZON PLANNING. SELF-REFLECTION ALLOWS AGENTS TO MONITOR THEIR OWN PERFORMANCE AND IDENTIFY AREAS FOR IMPROVEMENT.',\n",
              " 'MEMORY, THE ABILITY TO STORE AND RETRIEVE INFORMATION, IS ESSENTIAL FOR COGNITIVE FUNCTION. HUMAN MEMORY CAN BE CLASSIFIED INTO THREE MAIN TYPES:\\n\\n1. **SENSORY MEMORY:** RETAINS SENSORY INFORMATION FOR A FEW SECONDS (E.G., ICONIC MEMORY FOR VISUAL STIMULI, ECHOIC MEMORY FOR AUDITORY).\\n\\n2. **SHORT-TERM MEMORY (WORKING MEMORY):** STORES INFORMATION CURRENTLY IN USE, HOLDING A LIMITED AMOUNT (AROUND 7 ITEMS) FOR 20-30 SECONDS.\\n\\n3. **LONG-TERM MEMORY:** CAN STORE UNLIMITED INFORMATION FOR LONG DURATIONS. SUBDIVIDED INTO:\\n   - EXPLICIT (DECLARATIVE) MEMORY: CONSCIOUSLY RECALLED MEMORIES OF FACTS AND EVENTS (EPISODIC AND SEMANTIC MEMORY).\\n   - IMPLICIT (PROCEDURAL) MEMORY: UNCONSCIOUS MEMORIES OF SKILLS AND ROUTINES.',\n",
              " 'MAXIMUM INNER PRODUCT SEARCH (MIPS) IS A TECHNIQUE USED TO EFFICIENTLY RETRIEVE SIMILAR ITEMS FROM A LARGE VECTOR STORE. IT IS COMMONLY EMPLOYED IN NATURAL LANGUAGE PROCESSING MODELS TO ACCESS LONG-TERM MEMORY. MIPS INVOLVES STORING EMBEDDING REPRESENTATIONS OF INFORMATION IN A VECTOR DATABASE AND UTILIZING APPROXIMATE NEAREST NEIGHBOR (ANN) ALGORITHMS TO FIND THE TOP K NEAREST NEIGHBORS, SACRIFICING SOME ACCURACY FOR SIGNIFICANT SPEED ENHANCEMENTS. ANN ALGORITHMS, SUCH AS HNSW AND FAISS, ARE WIDELY USED FOR FAST MIPS. MIPS PLAYS A CRUCIAL ROLE IN ALLEVIATING THE FINITE ATTENTION SPAN LIMITATIONS OF MODELS AND ENABLES THEM TO LEVERAGE EXTERNAL MEMORY FOR IMPROVED PERFORMANCE.']"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without RunnableLambda"
      ],
      "metadata": {
        "id": "ViZ5qyDDalIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def string_upper(input: str) -> str:\n",
        "    return input.upper()\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | google_llm\n",
        "    | StrOutputParser()\n",
        "    | string_upper\n",
        ")\n",
        "\n",
        "chain.batch(['Task Decomposition', 'Types of Memory','Maximum Inner Product Search (MIPS)'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvSGkEN8ak0q",
        "outputId": "89f4c325-483e-4872-99ce-9ebdfd0d4377"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. TASK DECOMPOSITION IS CRUCIAL FOR LLM-POWERED AUTONOMOUS AGENTS TO PLAN AND EXECUTE COMPLEX TASKS.\\n2. TECHNIQUES FOR TASK DECOMPOSITION INCLUDE CHAIN OF THOUGHT (COT), TREE OF THOUGHTS, AND EXTERNAL CLASSICAL PLANNERS.\\n3. COT AND TREE OF THOUGHTS GUIDE THE LLM TO BREAK DOWN TASKS INTO SMALLER, MANAGEABLE STEPS.\\n4. EXTERNAL CLASSICAL PLANNERS, LIKE LLM+P, PROVIDE AN ALTERNATIVE APPROACH BY TRANSLATING THE PROBLEM INTO A FORMAT SUITABLE FOR PLANNING TOOLS.\\n5. TASK DECOMPOSITION CAN BE ACHIEVED THROUGH LLM PROMPTING, TASK-SPECIFIC INSTRUCTIONS, OR HUMAN INPUT.',\n",
              " '1. MEMORY INVOLVES ACQUIRING, STORING, RETAINING, AND RETRIEVING INFORMATION.\\n2. SENSORY MEMORY RETAINS SENSORY IMPRESSIONS FOR A FEW SECONDS (ICONIC, ECHOIC, HAPTIC).\\n3. SHORT-TERM MEMORY HOLDS INFORMATION FOR 20-30 SECONDS, WITH A LIMITED CAPACITY.\\n4. LONG-TERM MEMORY STORES INFORMATION FOR EXTENDED PERIODS, WITH UNLIMITED CAPACITY.\\n5. LONG-TERM MEMORY INCLUDES EXPLICIT (FACTS, EVENTS) AND IMPLICIT (SKILLS, ROUTINES) SUBTYPES.',\n",
              " \"1. MIPS (MAXIMUM INNER PRODUCT SEARCH) ENABLES EXTERNAL MEMORY ACCESS, EXTENDING THE AGENT'S FINITE ATTENTION SPAN.\\n2. MIPS INVOLVES STORING EMBEDDING REPRESENTATIONS IN A VECTOR STORE DATABASE FOR FAST RETRIEVAL.\\n3. APPROXIMATE NEAREST NEIGHBORS (ANN) ALGORITHMS ARE COMMONLY USED FOR MIPS, PROVIDING A BALANCE BETWEEN SPEED AND ACCURACY.\\n4. MIPS FACILITATES EFFICIENT SEARCHING FOR THE TOP NEAREST NEIGHBORS, ENHANCING THE RETRIEVAL OF RELEVANT INFORMATION.\\n5. MIPS ALGORITHMS VARY IN PERFORMANCE, WITH POPULAR CHOICES INCLUDING FAISS AND HNSW, AS EVIDENCED BY BENCHMARKS.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RunnableParallel"
      ],
      "metadata": {
        "id": "Jx2A-NlIa717"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template1 = \"\"\"Provide me the 100 words summary of the following topic from the context.\n",
        "{context}\n",
        "\n",
        "topic : {topic}\n",
        "\"\"\"\n",
        "prompt1 = ChatPromptTemplate.from_template(template1)\n",
        "\n",
        "chain1= (\n",
        "    {\"context\": retriever, \"topic\": RunnablePassthrough()}\n",
        "    | prompt1\n",
        "    | google_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "hd6-qHtXj915"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template2 = \"\"\"Provide me the 5 advantage of the following\n",
        "topic based on your understanding.\n",
        "\n",
        "Output should be in Json format\n",
        "topic : {topic}\n",
        "\"\"\"\n",
        "prompt2 = ChatPromptTemplate.from_template(template2)\n",
        "\n",
        "chain2= (\n",
        "    prompt2\n",
        "    | google_llm\n",
        "    | JsonOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "MdEtnlBXkUVT"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template3 = \"\"\"What is the advantage of {topic}\"\n",
        "\"\"\"\n",
        "prompt3 = ChatPromptTemplate.from_template(template3)\n",
        "\n",
        "chain3= (\n",
        "    prompt3\n",
        "    | google_llm\n",
        "    | StrOutputParser()\n",
        "    | RunnableLambda(string_upper)\n",
        ")\n"
      ],
      "metadata": {
        "id": "VuSZtxM-k8l_"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined = RunnableParallel(from_context=chain1, from_llm_knowledge=chain2, advantage = chain3)\n",
        "\n",
        "combined.invoke('Decomposition')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH-Lwz88a7Lr",
        "outputId": "b91ec000-3303-4f6c-b8c0-2b7d7d3999c0"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'from_context': '**Planning in Autonomous Agents using LLMs**\\n\\nLarge Language Models (LLMs) are increasingly used as the core controllers in autonomous agents. Planning is a key component of these agents, enabling them to break down complex tasks into manageable subgoals.\\n\\n**Subgoal and Decomposition**\\n\\nLLMs can use techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) to decompose tasks into smaller steps. This allows them to handle complex tasks efficiently.\\n\\n**Reflection and Refinement**\\n\\nAgents can also engage in self-criticism and self-reflection to refine their plans. They can learn from past mistakes and improve the quality of their future actions.\\n\\n**Task Decomposition Approaches**\\n\\nLLMs can decompose tasks using various methods:\\n\\n* Simple prompting (e.g., \"Steps for XYZ\")\\n* Task-specific instructions (e.g., \"Write a story outline\")\\n* External classical planners (e.g., using PDDL)\\n\\nBy incorporating planning into autonomous agents, LLMs enable them to tackle complex tasks effectively and efficiently.',\n",
              " 'from_llm_knowledge': {'Advantages of Decomposition': [{'Improved Modularity': 'Decomposing a system into smaller, independent modules makes it easier to manage and maintain the codebase.'},\n",
              "   {'Enhanced Reusability': 'Reusable components can be easily shared across multiple projects, saving development time and effort.'},\n",
              "   {'Increased Testability': 'Smaller, isolated modules are easier to test individually, improving overall code quality.'},\n",
              "   {'Reduced Complexity': 'Breaking down a complex system into smaller parts makes it easier to understand and debug.'},\n",
              "   {'Improved Scalability': 'Modular decomposition allows for easier scaling of the system by adding or removing components as needed.'}]},\n",
              " 'advantage': '**DECOMPOSITION** IS A DESIGN PRINCIPLE USED IN SOFTWARE ENGINEERING TO BREAK DOWN A COMPLEX PROBLEM OR SYSTEM INTO SMALLER, MORE MANAGEABLE PIECES. IT OFFERS SEVERAL ADVANTAGES:\\n\\n**1. MODULARITY:**\\n* DECOMPOSED MODULES ARE INDEPENDENT AND CAN BE DEVELOPED, TESTED, AND MAINTAINED SEPARATELY.\\n* THIS MAKES IT EASIER TO CHANGE OR UPDATE ONE MODULE WITHOUT AFFECTING THE REST OF THE SYSTEM.\\n\\n**2. REUSABILITY:**\\n* DECOMPOSED MODULES CAN BE REUSED IN OTHER PARTS OF THE SYSTEM OR IN DIFFERENT PROJECTS.\\n* THIS REDUCES CODE DUPLICATION AND SAVES DEVELOPMENT TIME.\\n\\n**3. MAINTAINABILITY:**\\n* SMALLER, WELL-DEFINED MODULES ARE EASIER TO UNDERSTAND AND MAINTAIN.\\n* CHANGES OR BUG FIXES CAN BE LOCALIZED TO SPECIFIC MODULES, REDUCING THE RISK OF UNINTENDED CONSEQUENCES.\\n\\n**4. TESTABILITY:**\\n* DECOMPOSED MODULES CAN BE TESTED INDEPENDENTLY, MAKING IT EASIER TO ISOLATE AND FIX BUGS.\\n* THIS IMPROVES THE OVERALL RELIABILITY OF THE SYSTEM.\\n\\n**5. SCALABILITY:**\\n* DECOMPOSED MODULES CAN BE SCALED INDEPENDENTLY, ALLOWING THE SYSTEM TO GROW OR SHRINK AS NEEDED.\\n* THIS MAKES IT EASIER TO HANDLE CHANGING REQUIREMENTS OR WORKLOADS.\\n\\n**6. PARALLELISM:**\\n* DECOMPOSED MODULES CAN BE EXECUTED IN PARALLEL, IMPROVING THE PERFORMANCE OF THE SYSTEM.\\n* THIS IS ESPECIALLY USEFUL FOR COMPLEX ALGORITHMS OR DATA-INTENSIVE APPLICATIONS.\\n\\n**7. EXTENSIBILITY:**\\n* DECOMPOSED MODULES CAN BE EASILY ADDED OR REMOVED, MAKING IT EASY TO EXTEND THE SYSTEM WITH NEW FEATURES OR FUNCTIONALITY.\\n* THIS ALLOWS THE SYSTEM TO ADAPT TO CHANGING REQUIREMENTS WITHOUT MAJOR REWRITES.\\n\\n**8. CODE READABILITY:**\\n* DECOMPOSED MODULES RESULT IN CLEANER AND MORE ORGANIZED CODE.\\n* THIS MAKES IT EASIER FOR DEVELOPERS TO UNDERSTAND AND COLLABORATE ON THE PROJECT.\\n\\n**9. REDUCED COMPLEXITY:**\\n* BREAKING DOWN A COMPLEX PROBLEM INTO SMALLER PIECES REDUCES ITS OVERALL COMPLEXITY.\\n* THIS MAKES IT EASIER TO DESIGN, IMPLEMENT, AND MANAGE THE SYSTEM.\\n\\n**10. IMPROVED COMMUNICATION:**\\n* DECOMPOSED MODULES PROVIDE A CLEAR STRUCTURE FOR COMMUNICATION AMONG TEAM MEMBERS.\\n* THIS HELPS TO AVOID MISUNDERSTANDINGS AND ENSURES THAT EVERYONE IS ON THE SAME PAGE.'}"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asynchronous\n",
        "\n",
        "Asynchronous in LCEL (LangChain Expression Language) refers to the ability to run chains and components concurrently, without blocking other operations. This is especially useful in LangChain when you're working with multiple LLM (Language Model) calls, retrievers, or external API calls that might take time to complete."
      ],
      "metadata": {
        "id": "MDEmOND1tZ1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "gqs1FHtDtxtU"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ainvoke"
      ],
      "metadata": {
        "id": "Hre-yTMWtHIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Invoke\n",
        "template = \"\"\"Provide me the 5 summary line of the following topic from the context.\n",
        "{context}\n",
        "\n",
        "topic : {topic}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "## StrOutputParser + Split the line function\n",
        "chain = prompt | google_llm | StrOutputParser() | (lambda x : x.split('\\n'))\n",
        "\n",
        "context_ = retriever.get_relevant_documents (\"Task Decomposition\")\n",
        "\n",
        "# asyncio.run(chain.ainvoke({'context' : context_,'topic' : 'Task Decomposition'}))\n",
        "await chain.ainvoke({'context' : context_,'topic' : 'Task Decomposition'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cJuj6NFt4I5",
        "outputId": "e4970830-3588-4284-cbde-a8319292ca8b"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. Task decomposition involves breaking down complex tasks into smaller, manageable steps.',\n",
              " '2. Chain of Thought (CoT) and Tree of Thoughts (ToT) are prompting techniques that guide models in decomposing tasks.',\n",
              " '3. Task decomposition can be performed by the LLM itself, using task-specific instructions, or with human input.',\n",
              " '4. LLM+P approach outsources planning to an external classical planner, utilizing Planning Domain Definition Language (PDDL).',\n",
              " '5. Task decomposition enables LLM-powered agents to plan and execute complex tasks more effectively.']"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "abatch"
      ],
      "metadata": {
        "id": "atj7mlc7u29H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## StrOutputParser + Split the line function\n",
        "chain = prompt | google_llm | StrOutputParser() | (lambda x: x.split('\\n'))\n",
        "\n",
        "context_sensory_memory = retriever.get_relevant_documents (\"Sensory Memory\")\n",
        "context_short_term_memory = retriever.get_relevant_documents (\"Short-Term Memory\")\n",
        "context_long_term_memory = retriever.get_relevant_documents (\"Long-Term Memory\")\n",
        "context_mips = retriever.get_relevant_documents (\"Maximum Inner Product Search (MIPS)\")\n",
        "\n",
        "await chain.abatch([\n",
        "    {'context' : context_sensory_memory,'topic' : 'Sensory Memory'},\n",
        "    {'context' : context_short_term_memory,'topic' : 'Short-Term Memory'},\n",
        "    {'context' : context_long_term_memory,'topic' : 'Long-Term Memory'},\n",
        "    {'context' : context_mips,'topic' : 'Maximum Inner Product Search (MIPS)'},\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfdJB-6SuiRu",
        "outputId": "ecb453fc-ddcc-4950-b3b6-ee15c6164fb1"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['1. Sensory memory is the initial stage of memory, retaining sensory information for a few seconds.',\n",
              "  '2. Subcategories of sensory memory include iconic (visual), echoic (auditory), and haptic (touch) memory.',\n",
              "  '3. Sensory memory is comparable to learning embedding representations for raw inputs in machine learning.',\n",
              "  '4. Sensory memory provides the foundation for short-term and long-term memory formation.',\n",
              "  '5. The duration of sensory memory is typically limited to a few seconds or less.'],\n",
              " ['1. Human memory can be categorized into sensory, short-term, and long-term memory.',\n",
              "  '2. Sensory memory retains sensory information for a few seconds, while short-term memory stores information needed for immediate cognitive tasks.',\n",
              "  '3. Long-term memory has unlimited storage capacity, and can store information for prolonged periods, including declarative (facts and events) and procedural (skills and routines) memories.',\n",
              "  '4. External memory, accessible via fast retrieval, can alleviate the finite attention span of short-term memory.',\n",
              "  '5. Maximum Inner Product Search (MIPS) algorithms are used to retrieve information from external memory efficiently.'],\n",
              " ['1. Human memory is categorized into sensory memory, short-term memory (STM), and long-term memory (LTM).',\n",
              "  '2. LTM stores information for extended durations, ranging from days to decades, with a vast storage capacity.',\n",
              "  '3. Explicit LTM encompasses episodic memory (events) and semantic memory (facts), while implicit LTM involves automatic skills and routines.',\n",
              "  '4. In artificial intelligence, LTM is analogized to an external vector store that can be accessed through fast Maximum Inner Product Search (MIPS).',\n",
              "  '5. Approximate Nearest Neighbor (ANN) algorithms are commonly used for MIPS to efficiently retrieve information from the vector store.'],\n",
              " ['1. MIPS is used to accelerate retrieval from an external memory, alleviating the limitations of finite attention span.',\n",
              "  '2. MIPS involves storing embedding representations of information in a vector store database.',\n",
              "  '3. Approximate Nearest Neighbors (ANN) algorithms are commonly employed for fast MIPS, balancing accuracy with speed.',\n",
              "  '4. MIPS enables agents to attend to information in long-term memory during query time.',\n",
              "  '5. ANN algorithms such as HNSW, IVFFlat, and PCAHash can be used for efficient MIPS.']]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ikFASlpF1HLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}